\documentclass[letter,8pt,landscape]{article}
\usepackage{fontspec}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel} 
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}


\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.1in,left=.1in,right=.1in,bottom=.1in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize

\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\vfill
\smallskip

  \section{General Concepts}
  Amdahl's law: $\text{overall speedup} = \frac{1}{(1-p) +
  \frac{p}{N}}$

  Gustafson's Law - Weak Scaling: $S = (1-p) + p \times N$

  Note that $N$: nproc and $p$: proportion of parallel code. Gustafson's Law
  addresses fixed problem size of Amdahl's law and instead gives the theoretical
  speedup a task gains from being parallelized. predacated that serial code in a
  parallelized environment remains fixed as problem scales as such assumes that
  problem scales with number of workers. Countering of serial component by
  increasing volume of computation. Amdahl's law is predicated under the strong
  scaling model where the problem size remains fixed.

  Little's Law: $L = p \times \lambda$
  
  Based on the idea of how to saturate compute volume. hid latency while
  performing other things during wait eg. memory access or pipeline stall.

  $\text{Memory instructions} = \text{memory}_\text{bw} \times \text{memory}_\text{latency}$

  $\text{Arithmetic Instructions} = \text{arith}_\text{throughput} \times \text{arith}_\text{latency}$

  $\text{Concurrency} = \text{memory}_\text{bandwidth} \times \text{memory}_\text{latency}$

  \subsection{Speedup \& Efficiency}

  GPU speedup: $S_\text{GPU} = \frac{\text{fastest CPU completion
  time}}{\text{completion time on GPU}}$

  Parallel Compute speedup: $S_p = \frac{\text{single threaded
  completion}}{\text{parallel completion}}$

  Efficiency: $E_p = \frac{S_p}{p}$ where $p$: nproc


  Scaled Speedup: $PT_p = T_1 + T_o$, where $T_1$ is serial runtime and $T_o$ is
  parallel overhead.

  Isoefficiency: There is some function $f(P) = N$ that specifies the problem
  size ($N$) that keeps parallel efficiency constant. given that efficiency is
  given as $E = \frac{S}{P} = \frac{1}{1 + \frac{T_o}{T_1}} = \frac{1}{1 +
  \frac{T_o}{Wt_c}} $ with $T_1 = Wt_c$ where $W$: problem size in operations
  and $t_c$: time per op. we note that $T_o$ is a function of $P$ as such $W$
  should grow as a function of $P$ as well. eg. with Fat tree topology $T_o =
  O(\log(P))$ -- network diameter

  \subsubsection{Performance Prediction}
  Computation time: $T_\text{comp} = D(n) + \frac{W(n) - D(n)}{p}$, where $W(n)$
  is the number of work nodes and $D(n)$ is nodes on critical path. Ratio
  implies how parallelizability it is $\frac{W(n)}{D(n)}$. eg, Matrix multiply
  -- $W(n) = n$: number of multiplies -- $D(n) = \log(n)$: reduction add. Ratio
  shows massive potential for parallelization

  Compute time: $T(1, (m,n)) =$ time for best serial algorithm on a $m\times n$
  problem. $T(p, (m,n)) = T(1,(m,n)) + T_\text{comm}$: runtime on $p$ processes
  where $(m,n) = \frac{N}{(p,q)}$ with $p\times q$ being process grid on
  $N\times N$ problem.
  $T_\gamma(p, (m,n)) = \frac{T(P,(m,n))}{m\cdot n\cdot \text{N iterations}}$.
  
  Grind time: parallel performance ignoring communication time
  $T_\text{comm}$



  \subsubsection{Super Linear Speedup}
  when $S_p > P \implies \frac{S_p}{p} = E_p > 1$. Real super linear scaling
  exiss when $P$ scales things such as memory/cache sizes/bandwidth or yields
  more network bandwidth. Can be faked with poor algorithm or measurement
  technique. often suggests a better serial algorithm exists since $S_p =
  \frac{T_s}{T_p}; E_p = \frac{S_p}{p} > 1 \implies T_s > p\times T_p$ meaning
  $T_s$ is not optimal and there exists some serial that could perform the
  parallel component and take less time than original.


  \subsection{Roofline Model}
  Peak performance for each processor is intrinsic based on clock speed with
  bandwidth determining how steep the slop is. $q = \frac{\text{Flops}}{\text{Bytes}}$.
  Where "bytes" is amount of memory operations and flops being the computation
  amount.
  \begin{center}
    \includegraphics[width=1.5in]{images/roofline-eg.jpg}
  \end{center}
  

  \subsection{Memory Hierarchy}

  Layers of cache hierarchy are to tradeoff access speed and capacity.

  \begin{center}
    \includegraphics[width=\linewidth]{images/nway-cache.jpg}
  \end{center}
  3-way cache extends to $n$-way cache

  Cold miss: miss due to first access of data

  Capacity miss: miss due to cache being full

  Conflict miss: miss due to too many set aliases for memory addresses mapping
  to caches

  Temporal Cache locality: recent use of memory address

  Spatial Cache locality: use of a memory address is close to a previously used
  one

  \section{Matrix Multiply}
  Naive Matrix Mult: $q = \frac{2n^3}{n^3 + 3n^2}$. With $2n^3$: $n^3$ being
  iterating over elements of $A,B$ and $2$ from add and mult. $n^3$ comes from
  loading blocks of $B$ (hot loop), $n^2$ comes from loading blocks of $A$ and
  load and store to blocks of $C$.

  Blocking Matrix Mult: Same comp intensity as the naive just access pattern is
  more cache friendly. So performance ends up being some scalar fraction of
  Naive as $N$ increases.

  GOTOblas:
  \begin{center}
    \includegraphics[width=2in]{images/gotoblas.jpg}
  \end{center}
  $q = \frac{2n^3}{(2N + 2) * \frac{n^2}{L}}$. Where $N$ is size of matrix macro
  kernel and $L$ is size of cache line.

  Calculating q (computational intensity) | 
  Tiling/blocking/Packing |
  number of ops. in C = AB + C  for m,n,k |
  outer vs inner product


  \subsection{Parallel Matrix Multiply}

  Cannon’s algorithm (Tp and Ep) |
  Communication avoidance |
  Theoretical lower bound on communication (\# OF words moved and \# of
  messages). |
  Johnson’s 3D algorithm |
  2.5D algorithm

  \section{SIMD}

  Vectorization Stripmining |
  Control divergence |
  Loop carried dependencies

  \section{Multithreading}

  OpenMP – ideas behind |
  Parallelization of loops – types of dependencies (true, output, anti) |
  Pthreads – ideas behind |
  OpenMP and dataraces |
  Barriers, atomics, mutex |
  OpenMP reductions

  \section{Gpu Architecture}

  GPU Memory Hierarchy |
  GPU Thread Hierarchy |
  GPU warp scheduling |
  GPU thread mapping to warps |
  Matrix Multiply on GPU – techniques (e.g. tiling) |
  Estimated speedup from tiling |
  Parallel Speedup computation |
  Occupancy and interpretation of occupancy |
  GPU Memory Interleaving |
  GPU Memory Coalescing |
  GPU Bank access and bank conflict avoidance |
  Thread Level and Instruction Level Parallelism |
  Concept of Arithmetic Latency and Memory Latency in GPUs |
  Little’s Law and GPUs |
  Cusp behavior – TLP and ILP |
  CUDA compilation flow to achieve source code portability between generations |
  (nvcc->ptx->ptxas->binary) |
  Predication vs. branching, thread divergence |
  Stripmining in the context of GPUs |
  – pipelines |
  Pipeline hazards, dependencies  |
  CPU pipelines vs GPU pipelines |
  Emphasis on latency reduction, emphasis on throughput |
  Scoreboarding in a GPU – detect when dependencies are resolved |
  Control divergence in a GPU |
  Costs – why does it happen |
  Do all branches cause control divergence? |
  Reduction operation in a GPU and how to lessen control divergence

  \section{Message Passing}

  Bulk Synchronous Parallelism model |
  Stencil Methods |
  What kinds of problems are amenable to stencil methods |
  ghost cells |
  Basic MPI concepts : Rank, ordering guarantees/causality, synchronous and  |
  asynchronous models. Mpi communicators, mpi tags. |
  \verb|MPI_Bcast|, \verb|Scatter|, \verb|Gather| \verb|Reduce|, etc.  and why you might use them (you don’t 
  need to memorize their detailed behavior, but you should know generally when 
  you might use them). |
  Difference in user semantics for blocking and non-blocking send/recv (when you
  can use the buffers, what you have to do to synchronize non-blocking comms) |
  One-sided communications

  \subsection{MPI Perf modeling}

  $\text{TotalTime} = \gamma \times \#\text{ops}  +  \#\text{msgs} \times (\alpha +
  \beta^{-1}_\infty \times n)$  where $n$ is the msg size, $\alpha$ is message
  latency, $\beta_\infty$ is peak bandwidth and $\gamma$ is time per op.

  \subsection{Communication Parameters}

  Surface vs volume effect |
  Effect of higher dimension grid on surface to volume effect. |
  Effect of higher dimensions on cache locality.

  \section{Asics}

  How systolic array does matrix multiplication |
  Difference between Cannon’s algorithm and TPU’s systolic array |
  Why CISC instructions in the TPU

  \section{Higher Level Abstractions}

  Major differences between, pthreads, openMP, Cuda, MPI

  \section{Examples}
  





\end{multicols}

\end{document}

